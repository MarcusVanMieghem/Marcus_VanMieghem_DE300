{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1EckyI1xdOtjrc-bHw_e1iXzIoCwdRv5Z","authorship_tag":"ABX9TyP8Wbix5/1RY926xbSrzi28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Homework setup (file download, etc)"],"metadata":{"id":"f7jHG4d-PcJk"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"WzyyJ1CoMYlm","executionInfo":{"status":"ok","timestamp":1748315783901,"user_tz":300,"elapsed":50,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"outputs":[],"source":["!curl -O https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv\n","!curl -O https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv\n","!curl -O https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv\n","!curl -O https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv"]},{"cell_type":"code","source":["# Import Spark session to initialize and manage the Spark application\n","from pyspark.sql import SparkSession\n","\n","# Import NLTK and stopwords for text preprocessing\n","import nltk\n","from nltk.corpus import stopwords\n","\n","# Import PySpark functions for working with columns and defining UDFs\n","from pyspark.sql.functions import udf, col\n","\n","# Import data types for use with UDFs\n","from pyspark.sql.types import ArrayType, StringType\n","\n","# Import numerical and data processing libraries\n","import numpy as np\n","import pandas as pd\n","\n","# Import additional PySpark functions for transformations\n","import pyspark.sql.functions as F\n","from pyspark.sql.functions import col, explode, length, collect_list, when"],"metadata":{"id":"FZYsXgUBWdsB","executionInfo":{"status":"ok","timestamp":1748315788510,"user_tz":300,"elapsed":4612,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"id":"qVcXXxRkjZcl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748315788641,"user_tz":300,"elapsed":122,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}},"outputId":"ca2cdfe5-15d6-46ff-be4c-b83761109fc2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Task 1: ti-idf definition"],"metadata":{"id":"slfCaoFfWWqQ"}},{"cell_type":"code","source":["# Create or retrieve a Spark session for running Spark operations\n","spark = (SparkSession.builder\n","         .master(\"local[*]\")\n","         .appName(\"AG news\")\n","         .getOrCreate()\n","        )"],"metadata":{"id":"0G71RacoWsBm","executionInfo":{"status":"ok","timestamp":1748315798820,"user_tz":300,"elapsed":10177,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Load the cleaned AG News dataset into a Spark DataFrame from the local directory\n","agnews = spark.read.csv(\n","    \"agnews_clean.csv\",\n","    inferSchema=True,\n","    header=True\n",")\n","\n","# Import necessary functions and types\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import ArrayType, StringType\n","\n","# Convert the 'filtered' column from a JSON string to an array of strings\n","agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"],"metadata":{"id":"Fg-NiedgWaSD","executionInfo":{"status":"ok","timestamp":1748315815202,"user_tz":300,"elapsed":16375,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Task 1: Design the MapReduce functions for calculating the tf-idf measure."],"metadata":{"id":"HGuoH3QOyGTV"}},{"cell_type":"code","source":["# Count the frequency of each word across all documents\n","word_counts = (\n","    agnews\n","    .select(F.explode(F.col(\"filtered\")).alias(\"word\"))  # Flatten the 'filtered' array into individual words\n","    .groupBy(\"word\")                                     # Group by each unique word\n","    .count()                                             # Count occurrences of each word\n","    .orderBy(\"count\", ascending=False)                   # Sort words by frequency in descending order\n",")\n"],"metadata":{"id":"IUJ2HRapgl-C","executionInfo":{"status":"ok","timestamp":1748315815410,"user_tz":300,"elapsed":200,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["This shows that I should most likely remove words like 39 or 1 or quot or gt lt, as well as stop words. These are pretty meaningless. So I will remove:\n","\n","39, quot, u, b and lt and qt (these may have been greater than or less than). I will also get rid of AP and ruters as these are just news sources."],"metadata":{"id":"08ARLf4IiNsr"}},{"cell_type":"code","source":["# Load the standard list of English stopwords from NLTK\n","stopword_list = set(stopwords.words('english'))\n","\n","# Define a custom set of additional unwanted tokens specific to the dataset\n","custom_list = {\"39\", \"quot\", \"u\", \"b\", \"lt\", \"gt\", \"qt\", \"ap\", \"reuters\"}\n","\n","# Combine the NLTK stopwords with the custom list to form the full filter list\n","full_filter_list = stopword_list.union(custom_list)"],"metadata":{"id":"Ed5F9dmAje6F","executionInfo":{"status":"ok","timestamp":1748315815448,"user_tz":300,"elapsed":33,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["There are 207 words that I will remove as I go row by row through the dataset."],"metadata":{"id":"nCI6P_Ujj59m"}},{"cell_type":"code","source":["# Explode the filtered word list so each word appears in a separate row, keeping track of the document ID\n","words_df = agnews.select(\n","    col(\"_c0\").alias(\"doc_id\"),                             # Rename the document ID column\n","    explode(col(\"filtered\")).alias(\"word\")                  # Flatten the array of words into individual rows\n",")\n","\n","# Filter out stopwords and words with length <= 2\n","words_df_clean = words_df.filter(\n","    (~col(\"word\").isin(full_filter_list)) & (F.length(col(\"word\")) > 2)\n",")\n","\n","# Group cleaned words back into a list per document\n","agnews_cleaned = words_df_clean.groupBy(\"doc_id\") \\\n","    .agg(F.collect_list(\"word\").alias(\"filtered\"))"],"metadata":{"id":"L5MpyVCPkoAL","executionInfo":{"status":"ok","timestamp":1748315816374,"user_tz":300,"elapsed":928,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Re-explode the cleaned word list so each word from each document appears in its own row\n","words_df = agnews_cleaned.select(\n","    col(\"doc_id\"),\n","    explode(col(\"filtered\")).alias(\"word\")\n",")"],"metadata":{"id":"FLUHzqokicCi","executionInfo":{"status":"ok","timestamp":1748315816411,"user_tz":300,"elapsed":33,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Count how many times each word appears in each document\n","word_counts = words_df.groupBy(\"doc_id\", \"word\").count().withColumnRenamed(\"count\", \"word_count\")\n","\n","# Count the total number of words in each document\n","total_words = words_df.groupBy(\"doc_id\").count().withColumnRenamed(\"count\", \"total_words\")"],"metadata":{"id":"htQYq2xrk4fH","executionInfo":{"status":"ok","timestamp":1748315816561,"user_tz":300,"elapsed":144,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Join word-level and document-level counts to prepare for TF calculation\n","tf = word_counts.join(total_words, \"doc_id\")\n","\n","# Calculate term frequency (TF) as word_count divided by total_words in the document\n","tf = tf.withColumn(\"tf\", col(\"word_count\") / col(\"total_words\"))"],"metadata":{"id":"KoZ0yNCsk72R","executionInfo":{"status":"ok","timestamp":1748315816953,"user_tz":300,"elapsed":385,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Calculate document frequency (DF) for each word:\n","# how many documents each word appears in (only once per document)\n","doc_freq = (\n","    words_df\n","    .dropDuplicates([\"doc_id\", \"word\"])  # Keep only one occurrence of each word per document\n","    .groupBy(\"word\")\n","    .count()\n","    .withColumnRenamed(\"count\", \"doc_freq\")\n",")"],"metadata":{"id":"qm6jiQ6ZnJZ2","executionInfo":{"status":"ok","timestamp":1748315817059,"user_tz":300,"elapsed":96,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Count the total number of documents\n","num_docs = agnews.count()\n","\n","# Calculate inverse document frequency (IDF) for each word\n","idf = doc_freq.withColumn(\"idf\", F.log(num_docs / F.col(\"doc_freq\")))"],"metadata":{"id":"ZWc_-7JCnhVj","executionInfo":{"status":"ok","timestamp":1748315818890,"user_tz":300,"elapsed":1819,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Task 2: Calculate tf-idf measure for each row in the agnews_clean.csv. Save the measures in a new column."],"metadata":{"id":"3qSsHIr-yDHD"}},{"cell_type":"code","source":["# Join the TF and IDF data to prepare for TF-IDF computation\n","tfidf = tf.join(idf, \"word\")\n","\n","# Calculate TF-IDF by multiplying term frequency with inverse document frequency\n","tfidf = tfidf.withColumn(\"tfidf\", col(\"tf\") * col(\"idf\"))"],"metadata":{"id":"HapbpD4YnlC9","executionInfo":{"status":"ok","timestamp":1748315819031,"user_tz":300,"elapsed":137,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## Task 3: Print out the tf-idf measure for the first 5 documents."],"metadata":{"id":"Pj-boYUbxex3"}},{"cell_type":"markdown","source":["Shown below are the TF-IDF scores for the first five documents, sorted by how important each word is in its document.\n","These scores tell us which words are the most unique and meaningful in each article. For example, in document 0, the word “cynics” has a high score because it doesn’t show up in many other documents, even though it only appears once. On the other hand, words like “black” or “wall” appear more often across the dataset, so their scores are lower. TF-IDF helps highlight the key terms that make each document stand out."],"metadata":{"id":"FY_fCUGUzMOO"}},{"cell_type":"code","source":["# Display all TF-IDF scores for the first 5 documents (doc_id < 5),\n","# sorted by document and then by TF-IDF score in descending order\n","tfidf.filter(col(\"doc_id\") < 5) \\\n","     .orderBy(\"doc_id\", \"tfidf\", ascending=[True, False]) \\\n","     .show(n=tfidf.count(), truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9B609Vfnm0R","executionInfo":{"status":"ok","timestamp":1748315884333,"user_tz":300,"elapsed":65293,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}},"outputId":"fb32980a-ac81-4ad0-cbbe-1388b6f408a3"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------+------+----------+-----------+--------------------+--------+------------------+-------------------+\n","|word          |doc_id|word_count|total_words|tf                  |doc_freq|idf               |tfidf              |\n","+--------------+------+----------+-----------+--------------------+--------+------------------+-------------------+\n","|cynics        |0     |1         |15         |0.06666666666666667 |5       |10.147217737458726|0.6764811824972484 |\n","|wall          |0     |2         |15         |0.13333333333333333 |1277    |4.604386793860288 |0.6139182391813717 |\n","|claw          |0     |1         |15         |0.06666666666666667 |16      |8.984066927653044 |0.5989377951768696 |\n","|dwindling     |0     |1         |15         |0.06666666666666667 |34      |8.230295125276665 |0.548686341685111  |\n","|sellers       |0     |1         |15         |0.06666666666666667 |41      |8.043083583188519 |0.5362055722125679 |\n","|ultra         |0     |1         |15         |0.06666666666666667 |76      |7.425922309606496 |0.4950614873070997 |\n","|seeing        |0     |1         |15         |0.06666666666666667 |143     |6.793811019632919 |0.4529207346421946 |\n","|band          |0     |1         |15         |0.06666666666666667 |181     |6.558158618627001 |0.4372105745751334 |\n","|bears         |0     |1         |15         |0.06666666666666667 |295     |6.069680293553007 |0.40464535290353376|\n","|black         |0     |1         |15         |0.06666666666666667 |627     |5.315709109259906 |0.35438060728399373|\n","|green         |0     |1         |15         |0.06666666666666667 |719     |5.17879429217178  |0.345252952811452  |\n","|short         |0     |1         |15         |0.06666666666666667 |867     |4.991616673112285 |0.3327744448741523 |\n","|street        |0     |1         |15         |0.06666666666666667 |1502    |4.442102817568746 |0.2961401878379164 |\n","|back          |0     |1         |15         |0.06666666666666667 |4233    |3.405989409371903 |0.22706596062479353|\n","|carlyle       |1     |2         |25         |0.08                |8       |9.67721410821299  |0.7741771286570392 |\n","|occasionally  |1     |1         |25         |0.04                |16      |8.984066927653044 |0.3593626771061218 |\n","|timed         |1     |1         |25         |0.04                |20      |8.760923376338836 |0.35043693505355344|\n","|bets          |1     |1         |25         |0.04                |69      |7.522549145295567 |0.3009019658118227 |\n","|aerospace     |1     |1         |25         |0.04                |120     |6.96916390711078  |0.2787665562844312 |\n","|reputation    |1     |1         |25         |0.04                |121     |6.960865104296086 |0.27843460417184346|\n","|quietly       |1     |1         |25         |0.04                |142     |6.800828592291566 |0.27203314369166265|\n","|placed        |1     |1         |25         |0.04                |267     |6.169406991492577 |0.24677627965970308|\n","|plays         |1     |1         |25         |0.04                |300     |6.052873175236625 |0.24211492700946502|\n","|controversial |1     |1         |25         |0.04                |446     |5.656336697872763 |0.22625346791491052|\n","|commercial    |1     |1         |25         |0.04                |493     |5.556146475850136 |0.22224585903400546|\n","|looks         |1     |1         |25         |0.04                |619     |5.328550377208231 |0.21314201508832922|\n","|private       |1     |1         |25         |0.04                |698     |5.208436547130454 |0.20833746188521815|\n","|toward        |1     |1         |25         |0.04                |757     |5.127292396455378 |0.2050916958582151 |\n","|investment    |1     |1         |25         |0.04                |774     |5.1050837763031   |0.20420335105212398|\n","|defense       |1     |1         |25         |0.04                |1128    |4.728454217834822 |0.1891381687133929 |\n","|well          |1     |1         |25         |0.04                |1277    |4.604386793860288 |0.1841754717544115 |\n","|making        |1     |1         |25         |0.04                |1300    |4.586536106443199 |0.18346144425772795|\n","|part          |1     |1         |25         |0.04                |1687    |4.325948567346858 |0.17303794269387432|\n","|firm          |1     |1         |25         |0.04                |1711    |4.311822376000633 |0.1724728950400253 |\n","|industry      |1     |1         |25         |0.04                |2197    |4.061807577508216 |0.16247230310032865|\n","|another       |1     |1         |25         |0.04                |2539    |3.9171300681881482|0.15668520272752592|\n","|market        |1     |1         |25         |0.04                |3429    |3.616631697429906 |0.14466526789719625|\n","|group         |1     |1         |25         |0.04                |4404    |3.366387152050256 |0.13465548608201025|\n","|outlook       |2     |2         |22         |0.09090909090909091 |764     |5.118087860726306 |0.4652807146114824 |\n","|doldrums      |2     |1         |22         |0.045454545454545456|15      |9.048605448790616 |0.4113002476723007 |\n","|economy       |2     |2         |22         |0.09090909090909091 |1467    |4.465680871749845 |0.40597098834089496|\n","|depth         |2     |1         |22         |0.045454545454545456|69      |7.522549145295567 |0.34193405205888944|\n","|hang          |2     |1         |22         |0.045454545454545456|85      |7.31400439340251  |0.33245474515465956|\n","|cloud         |2     |1         |22         |0.045454545454545456|107     |7.08382681543092  |0.32199212797413274|\n","|soaring       |2     |1         |22         |0.045454545454545456|251     |6.231202710761043 |0.2832364868527747 |\n","|plus          |2     |1         |22         |0.045454545454545456|361     |5.8677776915599456|0.26671716779817933|\n","|worries       |2     |1         |22         |0.045454545454545456|510     |5.522244924174455 |0.2510111329170207 |\n","|summer        |2     |1         |22         |0.045454545454545456|550     |5.44673737166631  |0.24757897143937774|\n","|crude         |2     |1         |22         |0.045454545454545456|1122    |4.733787563810185 |0.21517216199137204|\n","|earnings      |2     |1         |22         |0.045454545454545456|1727    |4.302514571746148 |0.19556884417027945|\n","|stock         |2     |1         |22         |0.045454545454545456|1747    |4.29100033975877  |0.195045469989035  |\n","|expected      |2     |1         |22         |0.045454545454545456|2681    |3.8627105116568674|0.1755777505298576 |\n","|market        |2     |1         |22         |0.045454545454545456|3429    |3.616631697429906 |0.16439234988317755|\n","|stocks        |2     |1         |22         |0.045454545454545456|3506    |3.5944245844116467|0.16338293565507486|\n","|prices        |2     |1         |22         |0.045454545454545456|3957    |3.4734142085074025|0.15788246402306375|\n","|next          |2     |1         |22         |0.045454545454545456|4366    |3.3750531127829375|0.15341150512649718|\n","|oil           |2     |1         |22         |0.045454545454545456|4531    |3.3379577052256884|0.15172535023753128|\n","|week          |2     |1         |22         |0.045454545454545456|5472    |3.14925619059044  |0.14314800866320182|\n","|pipeline      |3     |2         |26         |0.07692307692307693 |172     |6.609161173079373 |0.5083970133137979 |\n","|main          |3     |2         |26         |0.07692307692307693 |771     |5.108967276329497 |0.3929974827945767 |\n","|oil           |3     |3         |26         |0.11538461538461539 |4531    |3.3379577052256884|0.3851489659875795 |\n","|southern      |3     |2         |26         |0.07692307692307693 |1147    |4.711750532763456 |0.362442348674112  |\n","|flows         |3     |1         |26         |0.038461538461538464|54      |7.767671603328552 |0.29875660012802124|\n","|halts         |3     |1         |26         |0.038461538461538464|60      |7.662311087670726 |0.29470427260272025|\n","|halted        |3     |1         |26         |0.038461538461538464|99      |7.161535799758236 |0.27544368460608604|\n","|export        |3     |1         |26         |0.038461538461538464|160     |6.6814818346589995|0.25698007056380767|\n","|iraq          |3     |2         |26         |0.07692307692307693 |4552    |3.33333367408666  |0.2564102826220508 |\n","|infrastructure|3     |1         |26         |0.038461538461538464|206     |6.428779481103246 |0.24726074927320177|\n","|militia       |3     |1         |26         |0.038461538461538464|233     |6.305617196327126 |0.24252373832027407|\n","|exports       |3     |1         |26         |0.038461538461538464|313     |6.010452459352673 |0.23117124843664127|\n","|intelligence  |3     |1         |26         |0.038461538461538464|379     |5.8191194448104   |0.22381228633886155|\n","|rebel         |3     |1         |26         |0.038461538461538464|779     |5.098644604022079 |0.19610171553931074|\n","|authorities   |3     |1         |26         |0.038461538461538464|790     |5.08462270443176  |0.19556241170891384|\n","|showed        |3     |1         |26         |0.038461538461538464|968     |4.881423562616249 |0.187747060100625  |\n","|strike        |3     |1         |26         |0.038461538461538464|974     |4.875244346250292 |0.18750939793270355|\n","|official      |3     |1         |26         |0.038461538461538464|1835    |4.241855889404156 |0.16314830343862138|\n","|saturday      |3     |1         |26         |0.038461538461538464|4194    |3.4152454384309614|0.13135559378580622|\n","|said          |3     |1         |26         |0.038461538461538464|20141   |1.8461428324199454|0.07100549355461329|\n","|menace        |4     |2         |28         |0.07142857142857142 |23      |8.621161433963676 |0.6157972452831197 |\n","|tearaway      |4     |1         |28         |0.03571428571428571 |1       |11.756655649892826|0.41988055892474374|\n","|straining     |4     |1         |28         |0.03571428571428571 |21      |8.712133212169404 |0.3111476147203358 |\n","|toppling      |4     |1         |28         |0.03571428571428571 |29      |8.389359819906353 |0.29961999356808405|\n","|wallets       |4     |1         |28         |0.03571428571428571 |43      |7.995455534199264 |0.2855519833642594 |\n","|posing        |4     |1         |28         |0.03571428571428571 |54      |7.767671603328552 |0.2774168429760197 |\n","|afp           |4     |2         |28         |0.07142857142857142 |2746    |3.83875506356491  |0.27419679025463645|\n","|prices        |4     |2         |28         |0.07142857142857142 |3957    |3.4734142085074025|0.24810101489338587|\n","|soar          |4     |1         |28         |0.03571428571428571 |126     |6.920373742941348 |0.24715620510504813|\n","|oil           |4     |2         |28         |0.07142857142857142 |4531    |3.3379577052256884|0.23842555037326343|\n","|present       |4     |1         |28         |0.03571428571428571 |163     |6.662905449086065 |0.23796090889593086|\n","|barely        |4     |1         |28         |0.03571428571428571 |177     |6.580505917318997 |0.23501806847567847|\n","|records       |4     |1         |28         |0.03571428571428571 |340     |5.9277100322826195|0.21170392972437926|\n","|elections     |4     |1         |28         |0.03571428571428571 |1047    |4.80297143902229  |0.17153469425079607|\n","|economy       |4     |1         |28         |0.03571428571428571 |1467    |4.465680871749845 |0.15948860256249445|\n","|presidential  |4     |1         |28         |0.03571428571428571 |1504    |4.4407721453830415|0.15859900519225148|\n","|economic      |4     |1         |28         |0.03571428571428571 |1513    |4.434805936104471 |0.1583859262894454 |\n","|months        |4     |1         |28         |0.03571428571428571 |1912    |4.20075055628148  |0.1500268055814814 |\n","|new           |4     |2         |28         |0.07142857142857142 |18950   |1.9070964393822543|0.13622117424158958|\n","|record        |4     |1         |28         |0.03571428571428571 |3158    |3.698961455077239 |0.13210576625275852|\n","|time          |4     |1         |28         |0.03571428571428571 |5269    |3.1870597796835405|0.11382356356012643|\n","|three         |4     |1         |28         |0.03571428571428571 |5780    |3.0944966882264033|0.11051773886522868|\n","|world         |4     |1         |28         |0.03571428571428571 |7762    |2.799660337963975 |0.0999878692129991 |\n","+--------------+------+----------+-----------+--------------------+--------+------------------+-------------------+\n","\n"]}]},{"cell_type":"markdown","source":["# Task 2: SVM objective function"],"metadata":{"id":"j8jjmVzby_o0"}},{"cell_type":"markdown","source":["## Task 1: Design the MapReduce functions required to calculate the loss function.\n"],"metadata":{"id":"tfVVKEYoyOde"}},{"cell_type":"code","source":["# Load the full dataset (features and labels) from the local directory\n","data = pd.read_csv(\"data_for_svm.csv\", header=None)\n","\n","# Load the SVM weight vector and flatten it into a 1D array\n","w = pd.read_csv(\"w.csv\", header=None).values.flatten()\n","\n","# Load the bias term as a scalar\n","b = pd.read_csv(\"bias.csv\", header=None).values[0][0]\n","\n","# Split the dataset into feature matrix X and label vector y\n","X = data.iloc[:, :-1].values\n","y = data.iloc[:, -1].values"],"metadata":{"id":"4yCKtePwlFRH","executionInfo":{"status":"ok","timestamp":1748315887989,"user_tz":300,"elapsed":3660,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Task 2: Using these functions, create a function loss_SVM(w, b, X, y) to calculate the SVM objective for a given choice of w, b with data stored in X, y.\n"],"metadata":{"id":"DRIN9TcyyYgI"}},{"cell_type":"code","source":["# Compute the soft-margin SVM loss function\n","def loss_SVM(w, b, X, y, lam=0.1):\n","    reg_term = lam * np.linalg.norm(w) ** 2              # Regularization term: lambda * ||w||^2\n","    margins = 1 - y * (X @ w + b)                        # Compute margins: 1 - y_i * (w^T x_i + b)\n","    hinge_losses = np.maximum(0, margins)                # Apply hinge loss: max(0, margin)\n","    avg_hinge = np.mean(hinge_losses)                    # Average hinge loss over all examples\n","    total_loss = reg_term + avg_hinge                    # Total loss = regularization + average hinge loss\n","    return total_loss"],"metadata":{"id":"NxY8d7AJmHD8","executionInfo":{"status":"ok","timestamp":1748315888003,"user_tz":300,"elapsed":6,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Task 3: You are given the following dataset data_for_svm.csv, where the first 64 columns contain X and the last column contains y. Using the weights and bias provided in w.csv and bias.csv, calculate the objective value."],"metadata":{"id":"_0rqm_naybAn"}},{"cell_type":"code","source":["# Compute the SVM loss using the provided weights, bias, features, and labels\n","loss = loss_SVM(w, b, X, y)\n","\n","# Print the resulting loss value\n","print(\"SVM loss:\", loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNv2Jiz9nZd7","executionInfo":{"status":"ok","timestamp":1748315888228,"user_tz":300,"elapsed":219,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}},"outputId":"b72a95d6-706a-4867-c809-a368c8497802"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM loss: 1.0000454245191739\n"]}]},{"cell_type":"markdown","source":["The SVM loss is about 1. This means that given the provided weights and bias, the model has a relatively low combined loss. It indicates that while some predictions may fall within or beyond the margin (causing hinge loss), the overall error and regularization penalty are modest. This suggests the given weights and bias produce a reasonable fit to the data."],"metadata":{"id":"6YPH58h9yfz4"}},{"cell_type":"code","source":["# Load the dataset for prediction from the local directory\n","data_pred = spark.read.csv(\"data_for_svm.csv\", header=False, inferSchema=True)\n","\n","# Rename columns for clarity\n","for i in range(65):\n","    data_pred = data_pred.withColumnRenamed(f\"_c{i}\", f\"c{i}\")\n","\n","# Compute the dot product: w^T x + b\n","dot_product = sum([col(f\"c{i}\") * float(w[i]) for i in range(64)]) + float(b)\n","\n","# Apply SVM decision rule: sign(w^T x + b)\n","data_pred = data_pred.withColumn(\"prediction\", when(dot_product >= 0, 1).otherwise(-1))\n","\n","# Show first few predictions\n","data_pred.select(\"prediction\").show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxFQecAAoiQr","executionInfo":{"status":"ok","timestamp":1748315899276,"user_tz":300,"elapsed":11021,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}},"outputId":"e7cefb52-ebae-4159-c381-b240e988f74d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+\n","|prediction|\n","+----------+\n","|        -1|\n","|        -1|\n","|        -1|\n","|         1|\n","|        -1|\n","|         1|\n","|        -1|\n","|        -1|\n","|         1|\n","|        -1|\n","|         1|\n","|        -1|\n","|        -1|\n","|        -1|\n","|         1|\n","|         1|\n","|         1|\n","|        -1|\n","|         1|\n","|        -1|\n","+----------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"markdown","source":["Displayed above are the SVM predictions for the first 20 observations in the dataset."],"metadata":{"id":"k-CaR_3Ky1Z5"}},{"cell_type":"code","source":["# Write the DataFrame containing predictions to a CSV file named 'svm_predictions'\n","# This will create a directory with part files inside\n","data_pred.write.csv(\"svm_predictions\", header=True, mode=\"overwrite\")"],"metadata":{"id":"CmHpFmTWpaLo","executionInfo":{"status":"ok","timestamp":1748315913520,"user_tz":300,"elapsed":14243,"user":{"displayName":"Marcus Van Mieghem","userId":"15023073171385481682"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# AI disclosure"],"metadata":{"id":"B37ljkMezndh"}},{"cell_type":"markdown","source":["I used Generative Artificial Intelligence (ChatGPT) to support parts of the coding and documentation for this assignment. Specifically, I used ChatGPT to:\n","\n","- Add comments to my code for clarity and readability, especially for Spark transformations like withColumn, groupBy, and explode.\n","Prompt example: “Can you add basic comments to this PySpark code chunk that calculates TF-IDF?”\n","\n","- Confirming and editing the overall flow of my TF-IDF pipeline — including how to calculate term frequency (TF), document frequency (DF), inverse document frequency (IDF), and finally compute TF-IDF.\n","Prompt example: \"Can you help me build a TF-IDF pipeline using PySpark from tokenized text?”\n","\n","- Clarify the steps involved in computing TF-IDF using PySpark, and confirm that my approach matched MapReduce logic.\n","Prompt example: “Did I calculate TF-IDF for each row in the dataset and save the result in a new column?”\n","\n","- Help write the SVM loss function\n","Prompt example: “Am I writing this SVM loss function correctly?”"],"metadata":{"id":"o4O2orR4zsml"}}]}